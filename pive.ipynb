{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvLbqbp8a_6N"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, scale\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(1)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "tf.config.set_soft_device_placement(True)\n",
        "tf.debugging.set_log_device_placement(False)"
      ],
      "metadata": {
        "id": "ArAZG6f2gwtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0caa06b5-1bdf-4e7a-c794-004e1d4679d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zesl5S9GiKXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4490a456-692c-43c2-8737-e41c93726f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZrxNR89fID4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6f9a9f-d591-44f7-f155-147340825de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Lab/PIVE/backblaze\n"
          ]
        }
      ],
      "source": [
        "# cd /into/project/folder/\n",
        "cd /content/drive/MyDrive/Lab/PIVE/backblaze/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "uIG1-673KSIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = sorted(glob.glob(\"202*.csv\"))\n",
        "models = [\"ST12000NM0007\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\"]\n",
        "models_seen = set()\n",
        "for fname in files:\n",
        "  print(\"processing file -- {0}\".format(fname))\n",
        "  df = pd.read_csv(fname)\n",
        "  for model in models:\n",
        "    curr = df[df.model == model].copy()\n",
        "    \n",
        "    if model in models_seen:\n",
        "      curr.to_csv(\"models_logs/latest/{0}.csv\".format(model), mode='a', index=False, header=False)\n",
        "    else:\n",
        "      curr.to_csv(\"models_logs/latest/{0}.csv\".format(model), index=False)\n",
        "      models_seen.add(model)\n",
        "\n",
        "    print(\"model: {0}, size: {1}\".format(model, len(curr)))\n",
        "  print()"
      ],
      "metadata": {
        "id": "h-1w-J9MmA9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\"]\n",
        "\n",
        "columns = [\"date\", \"serial_number\"]\n",
        "features = [1, 4, 5, 7, 9, 12, 187, 188, 193, 194, 197, 198, 199]\n",
        "for feature in features:\n",
        "    columns.append(\"smart_{0}_raw\".format(feature))\n",
        "columns.append(\"label\")\n",
        "\n",
        "for model in models[:1]:\n",
        "  print(\"processing - model :: {0}\".format(model))\n",
        "\n",
        "  df_iterator = pd.read_csv(\"models_logs/{0}.csv\".format(model), chunksize=250000000)\n",
        "  # df = pd.read_csv(\"models_logs/{0}.csv\".format(model))\n",
        "  for i, df in enumerate(df_iterator):\n",
        "    print(\"\\t processing - chunk :: {0}\".format(i+1))\n",
        "    df = df.sort_values(by=[\"date\", \"serial_number\"])\n",
        "    df.smart_187_raw.fillna(0, inplace=True)\n",
        "    df[\"label\"] = df.groupby(\"serial_number\").smart_187_raw.diff(periods=-1).fillna(0)\n",
        "    df.loc[df.label != 0, \"label\"] = 1\n",
        "    df.label = df.label.astype(int)\n",
        "    df = df[columns]\n",
        "    df.dropna(inplace=True)\n",
        "    print(df.label.value_counts())\n",
        "    print()\n",
        "\n",
        "    # Set writing mode to append after first chunk\n",
        "    mode = 'w' if i == 0 else 'a'\n",
        "    \n",
        "    # Add header if it is the first chunk\n",
        "    header = i == 0\n",
        "    df.to_csv(\"dataset/{0}.csv\".format(model), index=False, header=header, mode=mode)"
      ],
      "metadata": {
        "id": "VFpy-7KgOrPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = sorted(glob.glob(\"2021*.csv\"))\n",
        "models = [\"ST12000NM0007\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\"]\n",
        "created = False\n",
        "\n",
        "for fname in files:\n",
        "  print(\"processing files -- {0}\".format(fname))\n",
        "  df = pd.read_csv(fname)\n",
        "  for model in models:\n",
        "    curr = df[df.model == model].copy()\n",
        "    if created:\n",
        "      curr.to_csv(\"models_logs/2021/combined.csv\".format(model), mode='a', index=False, header=False)\n",
        "    else:\n",
        "      curr.to_csv(\"models_logs/2021/combined.csv\".format(model), mode='w', index=False, header=True)\n",
        "      created = True\n",
        "\n",
        "    print(\"model: {0}, size: {1}\".format(model, len(curr)))\n",
        "  print()"
      ],
      "metadata": {
        "id": "srpcp92aQU3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd47424-95d3-4f2f-b13a-6a6544ad7284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing -- 2021q1.csv\n",
            "model: ST12000NM0007, size: 1732307\n",
            "model: ST8000NM0055, size: 1297674\n",
            "model: ST8000DM002, size: 878106\n",
            "model: ST4000DM000, size: 1701967\n",
            "\n",
            "processing -- 2021q2.csv\n",
            "model: ST12000NM0007, size: 726571\n",
            "model: ST8000NM0055, size: 1310887\n",
            "model: ST8000DM002, size: 885873\n",
            "model: ST4000DM000, size: 1714240\n",
            "\n",
            "processing -- 2021q3.csv\n",
            "model: ST12000NM0007, size: 216486\n",
            "model: ST8000NM0055, size: 1323645\n",
            "model: ST8000DM002, size: 895281\n",
            "model: ST4000DM000, size: 1724626\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = [1, 4, 5, 7, 9, 12, 187, 188, 193, 194, 197, 198, 199]\n",
        "columns = [\"date\", \"serial_number\"]\n",
        "for feature in features:\n",
        "    columns.append(\"smart_{0}_raw\".format(feature))\n",
        "columns.append(\"label\")\n",
        "\n",
        "models = [\"combined\"]\n",
        "for model in models:\n",
        "  print(\"processing - model :: {0}\".format(model))\n",
        "\n",
        "  df_iterator = pd.read_csv(\"models_logs/2021/{0}.csv\".format(model), chunksize=250000000)\n",
        "  # df = pd.read_csv(\"models_logs/{0}.csv\".format(model))\n",
        "  for i, df in enumerate(df_iterator):\n",
        "    print(\"\\t processing - chunk :: {0}\".format(i+1))\n",
        "    df = df.sort_values(by=[\"date\", \"serial_number\"])\n",
        "    df.smart_187_raw.fillna(0, inplace=True)\n",
        "    df[\"label\"] = df.groupby(\"serial_number\").smart_187_raw.diff(periods=-1).fillna(0)\n",
        "    df.loc[df.label != 0, \"label\"] = 1\n",
        "    df.label = df.label.astype(int)\n",
        "    df = df[columns]\n",
        "    df.dropna(inplace=True)\n",
        "    print(df.label.value_counts())\n",
        "    print()\n",
        "\n",
        "    # Set writing mode to append after first chunk\n",
        "    mode = 'w' if i == 0 else 'a'\n",
        "    \n",
        "    # Add header if it is the first chunk\n",
        "    header = i == 0\n",
        "    df.to_csv(\"dataset/{0}.csv\".format(model), index=False, header=header, mode=mode)"
      ],
      "metadata": {
        "id": "LlPjSyxHMEuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non DNN"
      ],
      "metadata": {
        "id": "TsDRew5sLVvz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrlfShhTWa_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f23d00-cad3-4b9c-a3cb-d9a39ec3239f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: ST3000DM001\n",
            "XGB -- AUC: 0.912\n",
            "Model: ST8000NM0055\n",
            "XGB -- AUC: 0.951\n",
            "Model: ST8000DM002\n",
            "XGB -- AUC: 0.958\n",
            "Model: ST4000DM000\n",
            "XGB -- AUC: 0.942\n",
            "Model: ST12000NM0007\n",
            "XGB -- AUC: 0.975\n",
            "Model: combined\n",
            "XGB -- AUC: 0.966\n"
          ]
        }
      ],
      "source": [
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\", \"combined\"]\n",
        "\n",
        "for model in models[:]:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  df.dropna(inplace=True)\n",
        "  y = df[\"label\"]\n",
        "  X = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.2)\n",
        "  X_train1, y_train1 = RandomUnderSampler(sampling_strategy=.33).fit_resample(X_train, y_train)\n",
        "  X_train, y_train = RandomUnderSampler(sampling_strategy=1).fit_resample(X_train, y_train)\n",
        "  X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(), XGBClassifier(n_estimators=125, max_depth=6, learning_rate=0.018, objective= 'binary:logistic', eval_metric=\"mlogloss\"))\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  print(\"XGB -- AUC:\", np.round(auc, 3))\n",
        "  with open(\"results/{0}_{1}.csv\".format(\"XGB\", model), \"w\") as ff:\n",
        "    ff.write(\"tpr,fpr\\n\")\n",
        "    for i in range(min(len(tpr),len(fpr))):\n",
        "      ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))\n",
        "\n",
        "\n",
        "  clf = make_pipeline(MinMaxScaler(), RandomForestClassifier(n_estimators=20))\n",
        "  clf.fit(X_train1, y_train1)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  print(\"Mahdisoltani -- AUC:\", np.round(auc, 3))\n",
        "  with open(\"results/{0}_{1}.csv\".format(\"Mahdisoltani\", model), \"w\") as ff:\n",
        "    ff.write(\"tpr,fpr\\n\")\n",
        "    for i in range(min(len(tpr),len(fpr))):\n",
        "      ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))\n",
        "\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=75))\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  print(\"RF -- AUC:\", np.round(auc, 3))\n",
        "  with open(\"results/{0}_{1}.csv\".format(\"RF\", model), \"w\") as ff:\n",
        "    ff.write(\"tpr,fpr\\n\")\n",
        "    for i in range(min(len(tpr),len(fpr))):\n",
        "      ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))\n",
        "\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(), MLPClassifier())\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  print(\"MLP -- AUC:\", np.round(auc, 3))\n",
        "  with open(\"results/{0}_{1}.csv\".format(\"DNN\", model), \"w\") as ff:\n",
        "    ff.write(\"tpr,fpr\\n\")\n",
        "    for i in range(min(len(tpr),len(fpr))):\n",
        "      ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))\n",
        "\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", probability=True))\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  print(\"SVC -- AUC:\", np.round(auc, 3))\n",
        "  with open(\"results/{0}_{1}.csv\".format(\"SVC\", model), \"w\") as ff:\n",
        "    ff.write(\"tpr,fpr\\n\")\n",
        "    for i in range(min(len(tpr),len(fpr))):\n",
        "      ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DNN"
      ],
      "metadata": {
        "id": "-x-P0wtVLeVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\", \"combined\"]\n",
        "\n",
        "for model in models[:1]:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  df.dropna(inplace=True)\n",
        "  y = df[\"label\"]\n",
        "  X = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.2)\n",
        "  X_train, y_train = RandomUnderSampler(sampling_strategy=1).fit_resample(X_train, y_train)\n",
        "  X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "  dnn = tf.keras.Sequential([\n",
        "                            #  tf.keras.Input(shape=(13,)),\n",
        "                             tf.keras.layers.Dense(128, activation='relu'),\n",
        "                             tf.keras.layers.Dense(64, activation='relu'),\n",
        "                             tf.keras.layers.Dense(32, activation='relu'),\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "                             ])\n",
        "\n",
        "  dnn.compile(\n",
        "      loss=tf.keras.losses.binary_crossentropy,\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=0.003),\n",
        "      metrics=[\n",
        "          tf.keras.metrics.AUC(name='AUC')\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  history = dnn.fit(X_train_scaled, y_train, epochs=25, verbose=0)\n",
        "  predictions = dnn.predict(X_test_scaled)\n",
        "  prediction_classes = [\n",
        "      1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n",
        "  ]\n",
        "\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  predictions)\n",
        "  auc = metrics.roc_auc_score(y_test, predictions)\n",
        "  print(\"AUC:\", np.round(auc* 100, 2))"
      ],
      "metadata": {
        "id": "VJGoC6_0d2cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN-LSTM"
      ],
      "metadata": {
        "id": "yXgUaYzKLgwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\", \"combined\"]\n",
        "\n",
        "for model in models:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  df.dropna(inplace=True)\n",
        "  y = df[\"label\"]\n",
        "  X = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.2)\n",
        "  X_train, y_train = RandomUnderSampler(sampling_strategy=1).fit_resample(X_train, y_train)\n",
        "  X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  n_features = X_train_scaled.shape[1]\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "  dnn = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Conv1D(filters=10, kernel_size=3, activation='relu', input_shape=(n_features, 1)),\n",
        "                             tf.keras.layers.Conv1D(filters=20, kernel_size=3, activation='relu'),\n",
        "                             tf.keras.layers.Dropout(rate=0.3),\n",
        "                             tf.keras.layers.MaxPool1D(),\n",
        "                            #  tf.keras.layers.Flatten(),\n",
        "                             tf.keras.layers.LSTM(300),\n",
        "                             tf.keras.layers.Dense(64, activation='relu'),\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "                             ])\n",
        "\n",
        "  dnn.compile(\n",
        "      loss=tf.keras.losses.binary_crossentropy,\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "      metrics=[\n",
        "          tf.keras.metrics.AUC(name='AUC')\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  history = dnn.fit(X_train_scaled, y_train, epochs=20, verbose=0)\n",
        "  predictions = dnn.predict(X_test_scaled)\n",
        "  prediction_classes = [\n",
        "      1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n",
        "  ]\n",
        "\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  predictions)\n",
        "  auc = metrics.roc_auc_score(y_test, predictions)\n",
        "  print(\"AUC:\", np.round(auc* 100, 2))\n",
        "\n",
        "  with open(\"results/{0}_{1}.csv\".format(\"CNNLSTM\", model), \"w\") as ff:\n",
        "    ff.write(\"tpr,fpr\\n\")\n",
        "    for i in range(min(len(tpr),len(fpr))):\n",
        "      ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))"
      ],
      "metadata": {
        "id": "yrzvtTS2Cp85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "WpKJTJ5zMaeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"ST3000DM001\", \"ST12000NM0007\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\"]\n",
        "ml_models = [\"dt\"] #, \"svc\", \"cnnlstm\",\"xgb\", \"mds\", \"rf\", \"dnn\"\n",
        "with open(\"results/dt_latest_auc.csv\", \"w\") as ff:\n",
        "  ff.write(\"disk_model,ml_model,period,pos_sample,neg_sample,auc,tpr,fpr\\n\")\n",
        "\n",
        "  for model in models:\n",
        "    print(\"Model: {0}\".format(model))\n",
        "    df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    for i in range(3):\n",
        "      if model == \"ST3000DM001\":\n",
        "        y = df[\"label\"]\n",
        "        X = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75)\n",
        "      else:\n",
        "        if i==0:\n",
        "          train = df[(df.date<\"2020-07-01\")].sort_values(by=\"date\")\n",
        "          test = df[(df.date>\"2020-06-30\") & (df.date<\"2020-10-01\")].sort_values(by=\"date\")\n",
        "        elif i==1:\n",
        "          train = df[(df.date<\"2020-10-01\")].sort_values(by=\"date\")\n",
        "          test = df[(df.date>\"2020-09-30\") & (df.date<\"2021-01-01\")].sort_values(by=\"date\")\n",
        "        else:\n",
        "          train = df[(df.date<\"2021-01-01\")].sort_values(by=\"date\")\n",
        "          test = df[(df.date>\"2020-12-31\")].sort_values(by=\"date\")\n",
        "\n",
        "        y_test = test[\"label\"]\n",
        "        X_test = test.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "        y_train = train[\"label\"]\n",
        "        X_train = train.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "      \n",
        "      X_train1, y_train1 = RandomUnderSampler(sampling_strategy=.33).fit_resample(X_train, y_train)\n",
        "      X_train, y_train = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_train, y_train)\n",
        "      # X_test, y_test = RandomUnderSampler(sampling_strategy=.05).fit_resample(X_test, y_test)\n",
        "\n",
        "      for ml in ml_models:\n",
        "        if ml != \"cnnlstm\" :\n",
        "          if ml == \"xgb\":\n",
        "            clf = make_pipeline(StandardScaler(),\n",
        "                              XGBClassifier(\n",
        "                                  n_estimators=125, max_depth=6, learning_rate=0.018, objective= 'binary:logistic', eval_metric=\"mlogloss\"))\n",
        "            clf.fit(X_train, y_train)\n",
        "\n",
        "          elif ml == \"mds\":\n",
        "            clf = make_pipeline(MinMaxScaler(), RandomForestClassifier(n_estimators=20))\n",
        "            clf.fit(X_train1, y_train1)\n",
        "          \n",
        "          elif ml == \"rf\":\n",
        "            clf = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=75))\n",
        "            clf.fit(X_train, y_train)\n",
        "          \n",
        "          elif ml == \"dt\":\n",
        "            clf = make_pipeline(StandardScaler(), tree.DecisionTreeClassifier())\n",
        "            clf.fit(X_train, y_train)\n",
        "\n",
        "          # elif ml == \"dnn\":\n",
        "          #   clf = make_pipeline(StandardScaler(), MLPClassifier())\n",
        "          #   clf.fit(X_train, y_train)\n",
        "  \n",
        "          else:\n",
        "            clf = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", probability=True))\n",
        "            clf.fit(X_train, y_train)\n",
        "\n",
        "          y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "        \n",
        "        elif ml == \"cnnlstm\":\n",
        "          # X_test, y_test = RandomUnderSampler(sampling_strategy=.05).fit_resample(X_test, y_test)\n",
        "          scaler = StandardScaler()\n",
        "          X_train_scaled = scaler.fit_transform(X_train)\n",
        "          n_features = X_train_scaled.shape[1]\n",
        "          X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "          dnn = tf.keras.Sequential([\n",
        "                                    tf.keras.layers.Conv1D(filters=10, kernel_size=3, activation='relu', input_shape=(n_features, 1)),\n",
        "                                    tf.keras.layers.Conv1D(filters=20, kernel_size=3, activation='relu'),\n",
        "                                    tf.keras.layers.Dropout(rate=0.3),\n",
        "                                    tf.keras.layers.MaxPool1D(),\n",
        "                                    tf.keras.layers.LSTM(300),\n",
        "                                    tf.keras.layers.Dense(64, activation='relu'),\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "                                    ])\n",
        "\n",
        "          dnn.compile(\n",
        "              loss=tf.keras.losses.binary_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "              metrics=[\n",
        "                  tf.keras.metrics.AUC(name='AUC')\n",
        "              ]\n",
        "          )\n",
        "\n",
        "          history = dnn.fit(X_train_scaled, y_train, epochs=25, verbose=0)\n",
        "          y_pred_proba = dnn.predict(X_test_scaled)\n",
        "        \n",
        "        else:\n",
        "          # X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "          scaler = StandardScaler()\n",
        "          X_train_scaled = scaler.fit_transform(X_train)\n",
        "          X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "          dnn = tf.keras.Sequential([\n",
        "                                    #  tf.keras.Input(shape=(13,)),\n",
        "                                    tf.keras.layers.Dense(128, activation='relu'),\n",
        "                                    tf.keras.layers.ReLU(),\n",
        "                                    tf.keras.layers.Dropout(rate=0.10),\n",
        "                                    tf.keras.layers.Dense(128, activation='relu'),\n",
        "                                    tf.keras.layers.ReLU(),\n",
        "                                    tf.keras.layers.Dense(1, activation='linear')\n",
        "                                    ])\n",
        "\n",
        "          dnn.compile(\n",
        "              loss=tf.keras.losses.binary_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.003),\n",
        "              metrics=[\n",
        "                  tf.keras.metrics.AUC(name='AUC')\n",
        "              ]\n",
        "          )\n",
        "\n",
        "          history = dnn.fit(X_train_scaled, y_train, epochs=25, verbose=0)\n",
        "          y_pred_proba = dnn.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "        fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "        temp = pd.DataFrame({\n",
        "              \"tpr\":tpr,\n",
        "              \"fpr\":fpr\n",
        "            })\n",
        "        \n",
        "        tpr, fpr = 0.9, np.round(temp[temp.tpr>=0.9].fpr.min(),4)\n",
        "        auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "        neg, pos = np.sum(y_train.values == 0), np.sum(y_train.values == 1)\n",
        "        row = \"{0},{1},{2},{3},{4},{5},{6},{7}\".format(model, ml, i+1, pos, neg, np.round(auc, 4), tpr, fpr)\n",
        "        print(row)\n",
        "        ff.write(row + \"\\n\")"
      ],
      "metadata": {
        "id": "NCS3UaNnGsys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Analysis"
      ],
      "metadata": {
        "id": "l1N4xVhkMfTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"ST8000NM0055\", \"ST4000DM000\", \"ST12000NM0007\"\n",
        "models = [\"ST8000DM002\"]\n",
        "\n",
        "for model in models[:]:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  train = df[(df.date>\"2019-12-31\") & (df.date<\"2020-12-01\")].sort_values(by=\"date\")\n",
        "  test = df[(df.date>\"2020-11-30\") & (df.date<\"2021-01-01\")].sort_values(by=\"date\")\n",
        "  del(df)\n",
        "\n",
        "  y_test = test[\"label\"]\n",
        "  X_test = test.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  y_train = train[\"label\"]\n",
        "  X_train = train.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  print(np.sum(y_train.values == 0), np.sum(y_train.values == 1))\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.2)\n",
        "  X_train, y_train = RandomUnderSampler(sampling_strategy=1).fit_resample(X_train, y_train)\n",
        "  print(np.sum(y_train.values == 0), np.sum(y_train.values == 1))\n",
        "  # X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(), XGBClassifier(n_estimators=125, max_depth=6, learning_rate=0.018, objective= 'binary:logistic', eval_metric=\"mlogloss\"))\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  print(\"XGB -- AUC:\", np.round(auc, 3))\n",
        "  with open(\"results/logs_{0}.csv\".format(model), \"w\") as ff:\n",
        "    ff.write(\"probability,original\\n\")\n",
        "    for i in range(len(X_test)):\n",
        "      ff.write(\"{0},{1}\\n\".format(y_pred_proba[i], y_test.iloc[i]))"
      ],
      "metadata": {
        "id": "h3Oa5Hf91GdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv(\"dataset/{0}.csv\".format(\"ST12000NM0007\"))\n",
        "# df.date.value_counts().plot()\n",
        "# metrics.recall_score(y_test, y_pred)\n",
        "models = [\"ST8000NM0055\", \"ST4000DM000\", \"ST12000NM0007\", \"ST8000DM002\"]\n",
        "for model in models:\n",
        "  print(model)\n",
        "  # df = pd.read_csv (\"results/logs2_{0}.csv\".format(model))\n",
        "  # fpr, tpr, _ = metrics.roc_curve(df.original,  df.probability)\n",
        "  # # print(model, len(df))\n",
        "  # temp = pd.DataFrame({\n",
        "  #     \"tpr\":tpr,\n",
        "  #     \"fpr\":fpr\n",
        "  # })\n",
        "  temp = pd.read_csv(\"results/XGB_{0}.csv\".format(model))\n",
        "  print(0.5, \"-\", np.round(temp[temp.tpr>=0.5].fpr.min(),3))\n",
        "  print(0.9, \"-\", np.round(temp[temp.tpr>=0.899].fpr.min(),3))\n",
        "  # print(0.95, \"-\", np.round(temp[temp.tpr>=0.95].fpr.min(),3))\n",
        "  # print(0.99, \"-\", np.round(temp[temp.tpr>=0.99].fpr.min(),3))\n",
        "  # print(0.999, \"-\", np.round(temp[temp.tpr>=0.999].fpr.min(),3))"
      ],
      "metadata": {
        "id": "vGIktCQs9PvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset/{0}.csv\".format(\"ST4000DM000\"))\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df[(df.date>\"2020-12-01\")]\n",
        "df1 = df.groupby(\"date\").label.sum().reset_index()\n",
        "df1.set_index(\"date\", inplace=True)\n",
        "df2 = df.groupby(\"date\").serial_number.count().reset_index()\n",
        "df2.set_index(\"date\", inplace=True)\n",
        "np.mean(df1.label/df2.serial_number)\n"
      ],
      "metadata": {
        "id": "MeUrEA50BCv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset/{0}.csv\".format(\"ST8000NM0055\"))\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df[(df.date<\"2020-12-01\")]\n",
        "df1 = df.groupby(\"date\").label.sum().reset_index()\n",
        "df1.set_index(\"date\", inplace=True)\n",
        "df2 = df.groupby(\"date\").serial_number.count().reset_index()\n",
        "df2.set_index(\"date\", inplace=True)\n",
        "np.mean(df1.label/df2.serial_number)\n",
        "# plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "WJe8lbmuBU8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset/{0}.csv\".format(\"ST12000NM0007\"))\n",
        "df['date'] = pd.to_datetime(df['date'])\n"
      ],
      "metadata": {
        "id": "4X-cH60TCRn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = df[(df.date<\"2021-01-01\")]\n",
        "df1 = df.groupby(\"date\").label.sum().reset_index()\n",
        "df1.set_index(\"date\", inplace=True)\n",
        "df2 = df.groupby(\"date\").serial_number.count().reset_index()\n",
        "df2.set_index(\"date\", inplace=True)\n",
        "np.mean(df1.label/df2.serial_number)"
      ],
      "metadata": {
        "id": "KgRm57O_C2ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"results/latest_auc.csv\")\n",
        "np.round(df.groupby([\"disk_model\",\"ml_model\"])[\"auc\", \"fpr\"].mean()*100, 2).sort_values(by=\"disk_model\")"
      ],
      "metadata": {
        "id": "x4Sw_3k-xh9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Feature scores\n",
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\", \"combined\"]\n",
        "score = {}\n",
        "for model in models:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  df.dropna(inplace=True)\n",
        "  y = df[\"label\"]\n",
        "  X = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75)\n",
        "  X_train, y_train = RandomUnderSampler(sampling_strategy=1).fit_resample(X_train, y_train)\n",
        "  X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "\n",
        "  clf = XGBClassifier(n_estimators=125, max_depth=6, learning_rate=0.018, objective= 'binary:logistic', eval_metric=\"mlogloss\")\n",
        "  clf.fit(X_train, y_train)\n",
        "  imp = np.round(clf.feature_importances_, 2)\n",
        "  print(imp)\n",
        "  columns = X.columns\n",
        "\n",
        "  for i in range(len(columns)):\n",
        "    if columns[i] in score:\n",
        "      score[columns[i]].append(imp[i])\n",
        "    else:\n",
        "      score[columns[i]] = [imp[i]]"
      ],
      "metadata": {
        "id": "NQ5AMstkBNu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\", \"combined\"]\n",
        "score2 = {}\n",
        "for model in models:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  df.dropna(inplace=True)\n",
        "  y = df[\"label\"]\n",
        "  X = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75)\n",
        "  # X_train, y_train = RandomUnderSampler(sampling_strategy=1).fit_resample(X_train, y_train)\n",
        "  # X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "\n",
        "  clf = RandomForestClassifier(n_estimators=10)\n",
        "  clf.fit(X, y)\n",
        "  imp = np.round(clf.feature_importances_, 4)\n",
        "  print(imp)\n",
        "  columns = X.columns\n",
        "\n",
        "  for i in range(len(columns)):\n",
        "    if columns[i] in score2:\n",
        "      score2[columns[i]].append(imp[i])\n",
        "    else:\n",
        "      score2[columns[i]] = [imp[i]]"
      ],
      "metadata": {
        "id": "t0zfeLOac55W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\", \"combined\"]\n",
        "\n",
        "for model in models[:]:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  df.dropna(inplace=True)\n",
        "  y = df[\"label\"]\n",
        "  X = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.2)\n",
        "  X_train1, y_train1 = RandomUnderSampler(sampling_strategy=.33).fit_resample(X_train, y_train)\n",
        "  X_train, y_train = RandomUnderSampler(sampling_strategy=1).fit_resample(X_train, y_train)\n",
        "  X_test, y_test = RandomUnderSampler(sampling_strategy=.1).fit_resample(X_test, y_test)\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(), XGBClassifier(n_estimators=125, max_depth=6, learning_rate=0.018, objective= 'binary:logistic', eval_metric=\"mlogloss\"))\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  print(\"XGB -- AUC:\", np.round(auc, 3))\n",
        "  # with open(\"results/{0}_{1}.csv\".format(\"XGB\", model), \"w\") as ff:\n",
        "  #   ff.write(\"tpr,fpr\\n\")\n",
        "  #   for i in range(min(len(tpr),len(fpr))):\n",
        "  #     ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))"
      ],
      "metadata": {
        "id": "TjQYCugpdSZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for key in score2:\n",
        "  scores.append(np.round(np.mean(score2[key]),4))\n",
        "  print(key, np.round(np.mean(score2[key]),4))\n",
        "\n",
        "print(scores)\n",
        "np.round(np.array(scores)/np.sum(scores),4)"
      ],
      "metadata": {
        "id": "zBOgqRPhl-xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Disk error ratio\n",
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\", \"ST12000NM0007\"]\n",
        "total = []\n",
        "for model in models:\n",
        "  disks = set()\n",
        "  errors = set()\n",
        "  double_errs = set()\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  with open(\"models_logs/{0}.csv\".format(model), \"r\") as ff:\n",
        "    line = ff.readline()\n",
        "    count = 1\n",
        "    while line:\n",
        "      if count>1:\n",
        "        row = line.split(\",\")\n",
        "        disks.add(row[1])\n",
        "        if row[10] != \"\" and  float(row[10])>0:\n",
        "          if row[1] in errors:\n",
        "            if row[1] not in double_errs:\n",
        "              double_errs.add(row[1])\n",
        "          else:\n",
        "            errors.add(row[1])  \n",
        "\n",
        "      line = ff.readline()\n",
        "      count += 1\n",
        "\n",
        "  total.append(count)\n",
        "  print(count,len(disks), len(errors), len(double_errs), np.round(len(errors)/len(disks), 4))\n",
        "\n",
        "print(np.sum(total))"
      ],
      "metadata": {
        "id": "qO5BocX2mhGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transfer Learning\n",
        "models = [\"ST3000DM001\", \"ST8000NM0055\", \"ST8000DM002\", \"ST4000DM000\"]\n",
        "df = pd.read_csv(\"dataset/{0}.csv\".format(\"ST12000NM0007\"))\n",
        "y_train = df[\"label\"]\n",
        "X_train = df.drop(columns=[\"date\", \"serial_number\", \"label\",])\n",
        "X_train, y_train = RandomUnderSampler(sampling_strategy=.33).fit_resample(X_train, y_train)\n",
        "del df\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), XGBClassifier(n_estimators=125, max_depth=6, learning_rate=0.018, objective= 'binary:logistic', eval_metric=\"mlogloss\"))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "for model in models:\n",
        "  print(\"Model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "  df.dropna(inplace=True)\n",
        "  y_test = df[\"label\"]\n",
        "  X_test = df.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  del df\n",
        "  # X_test, y_test = RandomUnderSampler(sampling_strategy=.01).fit_resample(X_test, y_test)\n",
        "  y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "  print(\"XGB -- AUC:\", np.round(auc, 3))\n",
        "  with open(\"results/{0}_ST12000NM0007_{1}.csv\".format(\"XGB\", model), \"w\") as ff:\n",
        "    ff.write(\"tpr,fpr\\n\")\n",
        "    for i in range(min(len(tpr),len(fpr))):\n",
        "      ff.write(\"{0},{1}\\n\".format(tpr[i],fpr[i]))\n"
      ],
      "metadata": {
        "id": "4qvZGtiZtbe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transfer Learning\n",
        "models = [\"ST4000DM000\", \"ST8000NM0055\", \"ST8000DM002\", \"ST12000NM0007\"]\n",
        "for model in models[3:]:\n",
        "  print(\"model: {0}\".format(model))\n",
        "  df = pd.read_csv(\"dataset/{0}.csv\".format(model))\n",
        "\n",
        "  test = df[(df.date>\"2020-11-30\") & (df.date<\"2021-01-01\")].sort_values(by=\"date\")\n",
        "  y_test = test[\"label\"]\n",
        "  X_test = test.drop(columns=[\"date\", \"serial_number\", \"label\"])\n",
        "  for i in range(11,0,-1):\n",
        "    print(\"\\tperiod: {0}\".format(i))\n",
        "    if i>9:\n",
        "      train = df[(df.date>=\"2020-{0}-01\".format(i)) & (df.date<\"2020-12-01\")].sort_values(by=\"date\")\n",
        "    else:\n",
        "      train = df[(df.date>=\"2020-0{0}-01\".format(i)) & (df.date<\"2020-12-01\")].sort_values(by=\"date\")\n",
        "\n",
        "    y_train = train[\"label\"]\n",
        "    X_train = train.drop(columns=[\"date\", \"serial_number\", \"label\",])\n",
        "    X_train, y_train = RandomUnderSampler(sampling_strategy=.33).fit_resample(X_train, y_train) \n",
        "    clf = make_pipeline(StandardScaler(), XGBClassifier(n_estimators=125, max_depth=6, learning_rate=0.018, objective= 'binary:logistic', eval_metric=\"mlogloss\"))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "    print(\"\\tXGB -- AUC:\", np.round(auc, 3))\n",
        "    temp = pd.DataFrame({\n",
        "        \"tpr\":tpr,\n",
        "        \"fpr\":fpr\n",
        "    })\n",
        "    print(\"\\ttpr=0.9 - fpr={0}\".format(np.round(temp[temp.tpr>=0.9].fpr.min(),3)))"
      ],
      "metadata": {
        "id": "EvuTNu8KJZ2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cWBnpcesMKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}